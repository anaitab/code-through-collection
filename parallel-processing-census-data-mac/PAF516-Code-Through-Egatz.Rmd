---
title: "Using Parallel Processing with Census Data in R"
author: "Ana Egatz Gomez"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: "show"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE
)
```

# 1. Why parallel processing?

Parallel processing can significantly reduce computation time by splitting independent tasks across multiple CPU cores. Since a Mac with an M2 chip has several cores, we can often speed up:

-   Repeated calculations on many units (tracts, counties, years)
-   Simulation or bootstrapping
-   Fitting the same model separately for many areas

But not all tasks benefit. We’ll come back to that; first, let’s get set up.

# 2. Setup: packages for parallel work

Base R ships with parallel, but we’ll use future + furrr for a friendlier workflow that plays nicely with tidyverse-style code.

```{r packages}
# Install once (uncomment if needed)

# install.packages("future")
# install.packages("furrr")
# install.packages("tidycensus")
# install.packages("dplyr")
# install.packages("purrr")

library(parallel)  # base R parallel processing
library(future)    # backend for parallel plans
library(furrr)     # parallel versions of purrr mappers
library(dplyr)
library(purrr)
library(tidycensus)
```

# 3. How many cores do you have?

Find out how many processing cores your machine has. In a Mac computer, click the Apple menu \> About This Mac \> General. Scroll down and click `System Report`.

Or, open Terminal and Type or paste the following command into the Terminal window and press Enter: `sysctl hw.physicalcpu hw.logicalcpu`

Read the output.

-   hw.physicalcpu: This line shows the number of physical CPU cores.
-   hw.logicalcpu: This line shows the number of logical cores.

# 4. How may cores should you use for paralell processing?

A rule of thumb is `(total number of cores) - 1`

# 5. Get some census data

For this example we’ll pull ACS 5-year data for census tracts for all the United States. You only need to run census_api_key() once per machine to store the key.

```{r}
# Set your Census API key once (then restart R or reload .Renviron)

# census_api_key("YOUR_CENSUS_KEY_GOES_HERE", install = TRUE)

# All states + DC
us_states <- c(state.abb, "DC")


variables <- c(
med_income = "B19013_001",  # median household income
med_value  = "B25077_001"   # median home value
)

ny_tracts <- get_acs(
geography = "tract",
state = us_states,
variables = variables,
year = 2022,
geometry = FALSE,
cache_table = TRUE
)

head(ny_tracts)
```

The data will be long: each row is a tract–variable combination. Let’s pivot to wide form (one row per tract) and extract county FIPS codes so we can treat each county as an independent “task”.

```{r}
ny_tracts_wide <- ny_tracts |>
select(GEOID, variable, estimate) |>
tidyr::pivot_wider(
names_from = variable,
values_from = estimate
)

ny_tracts_wide <- ny_tracts_wide |>
mutate(
county_fips = substr(GEOID, 1, 5)  # state+county code
)

length(unique(ny_tracts_wide$county_fips))

```

Each county will be one workload that we can process either sequentially or in parallel.

# 6. What kinds of tasks benefit from parallelism?

Parallel processing helps when:

-   You have many independent subtasks:
    -   e.g., “compute summary stats for each county”
    -   e.g., “fit the same regression for each metro area”
-   Each subtask is moderately expensive (e.g., 0.1–several seconds)
-   There’s little communication needed between subtasks

Parallel processing does not help much when:

-   The task is tiny (overhead dominates)
-   There is a strong sequential dependency (step t depends on step t–1)
-   The heavy work is already parallelized in optimized C/Fortran code
-   The bottleneck is I/O (reading/writing many files or slow network)

Think of parallelism as: “many copies of the same job that could be done on different cores at the same time”.

# 7. Base R: mclapply() example (Unix/macOS only)

`mclapply()` runs a function in parallel over a list or vector. It works only on Unix-like systems (macOS, Linux), not on Windows.

We’ll define a function that:

-   Filters the data to one county

-   Computes mean income and mean home value

-   Fits a simple regression of home value on income

```{r}
county_analysis <- function(cfips, data) {

# Simulate a moderately heavy task
# (e.g., more complex cleaning or model fitting)

d <- dplyr::filter(data, county_fips == cfips)

# If a county has very few tracts, skip

if (nrow(d) < 10) {
return(NULL)
}

# Simple summary

mean_income <- mean(d$med_income, na.rm = TRUE)
mean_value  <- mean(d$med_value,  na.rm = TRUE)

# Simple model (value ~ income)

mdl <- lm(med_value ~ med_income, data = d)

list(
county_fips  = cfips,
mean_income  = mean_income,
mean_value   = mean_value,
slope_income = coef(mdl)["med_income"]
)
}

county_list <- unique(ny_tracts_wide$county_fips)
length(county_list)

```

## Sequential vs parallel with mclapply()

```{r}
# Sequential version (lapply)

system.time({
res_seq <- lapply(county_list, county_analysis, data = ny_tracts_wide)
})

# Parallel version (mclapply)

n_cores <- max(1, detectCores() - 1)

system.time({
res_par <- mclapply(
county_list,
county_analysis,
data = ny_tracts_wide,
mc.cores = n_cores
)
})

# Bind results into a data frame

res_par_df <- purrr::compact(res_par) |>
purrr::map_dfr(~ as.data.frame(.x))
head(res_par_df)

```

### **1. user time**

Time spent by the CPU running the code.

### **2. system time**

Time spent in **system-level operations** (file access, memory allocation, OS calls).

### **3. elapsed time**

The **wall-clock** time — the total time you waited. This is the number you care about when comparing sequential vs parallel. **elapsed \< user time** indicates successful parallelization (multiple cores working at once)

# 7. `future` + `furrr`: tidyverse-friendly parallelism

The future framework separates what you want to do (map over a list) from how you parallelize it. furrr gives you future_map() and friends that work like purrr::map(), but run in parallel.

```{r}
# Sequential: purrr::map()

system.time({
res_seq2 <- map(county_list, county_analysis, data = ny_tracts_wide)
})

# Parallel: furrr::future_map()

system.time({
res_par2 <- future_map(
county_list,
county_analysis,
data = ny_tracts_wide,
.progress = TRUE  # nice progress bar
)
})

res_par2_df <- res_par2 |>
purrr::compact() |>
purrr::map_dfr(~ as.data.frame(.x))

head(res_par2_df)
```

Same logic as before, but syntax aligns with the rest of your tidyverse work. You can use `future_map_dfr()` if you always want a data frame bound row-wise.

# 8. When parallelism is not worth it

Parallel workers take time to start, copy data, and collect results back. If your function is extremely cheap, parallelism might actually be slower.

```{r}
cheap_function <- function(x) x + 1

x <- 1:5000

# Sequential

system.time({
cheap_seq <- map_dbl(x, cheap_function)
})

# Parallel

system.time({
cheap_par <- future_map_dbl(x, cheap_function)
})

```

You’ll likely see that the parallel version offers little or no benefit because each individual task is too small.

Rule of thumb: parallelizing micro-tasks usually wastes time.

# 9. Specific worked example: parallel models by county

Let’s put everything together in a clear, census-focused example.

Goal: Fit a tract-level model of median home value on median income separately for each county, and extract the income slope. We’ll use parallel processing to speed up the per-county fits.

9.1 Prepare the data

(Assumes ny_tracts_wide is already created as above.)

```{r}
# Drop rows with missing values for simplicity

ny_clean <- ny_tracts_wide |>
filter(
!is.na(med_income),
!is.na(med_value)
)

county_list <- unique(ny_clean$county_fips)
length(county_list)

```

9.2 Define the per-county model function

```{r}
county_model <- function(cfips, data) {
d <- filter(data, county_fips == cfips)

if (nrow(d) < 20) {
return(NULL)
}

# Add some artificial work to make the example more realistic

# (e.g., additional transformations or checks)

d <- d |>
mutate(
log_income = log(med_income),
log_value  = log(med_value)
)

mdl <- lm(log_value ~ log_income, data = d)

tibble::tibble(
county_fips = cfips,
n_tracts    = nrow(d),
slope       = coef(mdl)["log_income"],
r_squared   = summary(mdl)$r.squared
)
}

```

9.3 Compare sequential vs parallel

```{r}
# Sequential version

system.time({
county_results_seq <- map_dfr(
county_list,
county_model,
data = ny_clean
)
})

# Parallel version with future_map_dfr()

plan(multisession, workers = max(1, detectCores() - 1))

system.time({
county_results_par <- future_map_dfr(
county_list,
county_model,
data = ny_clean,
.progress = TRUE
)
})

head(county_results_par)

```

The parallel version should noticeably reduce elapsed time if the per-county work is heavy enough (more complex models, more variables, etc.).

# 9.4 Interpreting the results

```{r}
county_results_par |>
arrange(desc(slope)) |>
head(10)

```

You now have a data frame where:

-   Each row is a county

-   slope is the estimated elasticity of home values with respect to income

-   r_squared shows how well the log–log model explains variation

You can map these results, join them back to county geometries, or use them in further regressions.

# 10. Summary

Parallel processing is great when you have many independent, moderately heavy tasks (like per-county or per-tract analyses of census data). It is not helpful for tiny tasks, heavily sequential algorithms, or I/O-bound operations.
